{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358d7d7d",
   "metadata": {},
   "source": [
    "# TEXT EMOTION DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616dbc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepanshudubb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import neattext.functions as nfx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc506f",
   "metadata": {},
   "source": [
    "### import pandas as pd: \n",
    "- Pandas is a powerful data manipulation and analysis library in Python. It's commonly used for handling structured data, such as data from CSV files or databases.\n",
    "\n",
    "### import numpy as np:\n",
    "- NumPy is a fundamental package for numerical computations in Python. It provides support for arrays, matrices, and mathematical functions, making it essential for scientific computing.\n",
    "\n",
    "### import seaborn as sns:\n",
    "- Seaborn is a data visualization library built on top of Matplotlib. It provides a high-level interface for creating attractive statistical graphics.\n",
    "\n",
    "### import matplotlib.pyplot as plt:\n",
    "- Matplotlib is a versatile plotting library in Python. The pyplot module provides a MATLAB-like interface for creating plots and visualizations.\n",
    "\n",
    "### import neattext.functions as nfx:\n",
    "- Neattext is a library for text preprocessing and cleaning tasks. It offers functions to remove stopwords, clean text, handle emojis, and more.\n",
    "\n",
    "### from sklearn.model_selection import train_test_split: \n",
    "- Scikit-learn (sklearn) is a popular machine learning library in Python. The train_test_split function is used to split data into training and testing sets.\n",
    "\n",
    "### from sklearn.pipeline import Pipeline: \n",
    "- Scikit-learn's Pipeline class is used to chain multiple processing steps together, such as data preprocessing and model training.\n",
    "\n",
    "### from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer: \n",
    "- These classes from scikit-learn are used for text vectorization. CountVectorizer converts text documents to a matrix of token counts, while TfidfTransformer transforms a count matrix to a normalized term-frequency or term-frequency-inverse-document-frequency (TF-IDF) representation.\n",
    "\n",
    "### from sklearn.svm import SVC:\n",
    "- SVC stands for Support Vector Classifier, which is a type of support vector machine (SVM) used for classification tasks.\n",
    "\n",
    "### from sklearn.ensemble import RandomForestClassifier: \n",
    "- RandomForestClassifier is an ensemble learning method that fits multiple decision tree classifiers on various sub-samples of the dataset and uses averaging to improve predictive accuracy and control over-fitting.\n",
    "\n",
    "### from sklearn.cluster import KMeans: \n",
    "- KMeans is a clustering algorithm used for unsupervised learning tasks to partition data into clusters.\n",
    "\n",
    "### from sklearn.metrics import silhouette_score:\n",
    "- Silhouette score is a metric used to evaluate the quality of clusters created by clustering algorithms like KMeans.\n",
    "\n",
    "### from sklearn.linear_model import LogisticRegression:\n",
    "- Logistic regression is a linear model used for binary classification tasks.\n",
    "\n",
    "### from sklearn.metrics import classification_report, confusion_matrix: \n",
    "- These metrics are used to evaluate the performance of classification models.\n",
    "\n",
    "### from sklearn.naive_bayes import MultinomialNB:\n",
    "- Multinomial Naive Bayes is a probabilistic classifier commonly used for text classification tasks.\n",
    "\n",
    "### import nltk: \n",
    "- NLTK (Natural Language Toolkit) is a library for natural language processing tasks such as tokenization, stemming, lemmatization, and more.\n",
    "\n",
    "### from nltk.tokenize import word_tokenize: \n",
    "- Word tokenization is the process of splitting text into individual words or tokens.\n",
    "\n",
    "### from nltk.stem import WordNetLemmatizer:\n",
    "- Lemmatization is the process of reducing words to their base or root form.\n",
    "\n",
    "### nltk.download('wordnet'): \n",
    "- This line downloads the WordNet corpus, which is necessary for lemmatization using NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7ea18",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fdf46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r'/Users/deepanshudubb/Downloads/ML_Project/Data/emotion_dataset_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4841dd0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral                                             Why ? \n",
       "1      joy    Sage Act upgrade on my to do list for tommorow.\n",
       "2  sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...\n",
       "3      joy   Such an eye ! The true hazel eye-and so brill...\n",
       "4      joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d8cdf",
   "metadata": {},
   "source": [
    "prints the first few rows using head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b224913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "joy         11045\n",
       "sadness      6722\n",
       "fear         5410\n",
       "anger        4297\n",
       "surprise     4062\n",
       "neutral      2254\n",
       "disgust       856\n",
       "shame         146\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e54f0",
   "metadata": {},
   "source": [
    " The result of df['Emotion'].value_counts() is stored in the variable emotion_counts. The code then prints emotion_counts, which displays the count of each unique emotion in the 'Emotion' column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55053732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Emotion', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Emotion',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b577d",
   "metadata": {},
   "source": [
    "### Importing Seaborn: \n",
    "- The code assumes that seaborn has been imported previously using import seaborn as sns.\n",
    "\n",
    "### Creating the Count Plot:\n",
    "\n",
    "- sns.countplot(): This function creates a count plot, which is a type of bar plot that shows the counts of observations in each categorical bin using bars. In this case, the 'x' parameter specifies that the 'Emotion' column should be used for the x-axis of the plot.\n",
    "\n",
    "- x='Emotion': This sets the data to be plotted on the x-axis to be the 'Emotion' column from the DataFrame df.\n",
    "- data=df: This specifies the DataFrame from which the data for the plot will be taken.\n",
    "\n",
    "The count plot generated by this code will display bars representing the counts of each unique emotion in the 'Emotion' column of the DataFrame. Each bar's height corresponds to the frequency of that particular emotion in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44cdbf",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341f2520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/deepanshudubb/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1175202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neattext\n",
    "import neattext.functions as nfx\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = nfx.remove_userhandles(text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    cleaned_words = [word for word in lemmatized_words if len(word) > 2]  # Remove short words\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "df['Clean_Text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b059726",
   "metadata": {},
   "source": [
    "### !pip install neattext': \n",
    "- This line is a comment indicating that the NeatText library needs to be installed using pip, a package manager for Python.\n",
    "\n",
    "### import neattext.functions as nfx:\n",
    "- This line imports the NeatText library and renames it to nfx for easier use in the code.\n",
    "\n",
    "### lemmatizer = WordNetLemmatizer():\n",
    "- Here, we create an instance of the WordNetLemmatizer from the NLTK library. This lemmatizer is used to reduce words to their base or root form.\n",
    "\n",
    "### def preprocess_text(text):\n",
    "- This line defines a function named preprocess_text that takes a text input as an argument.\n",
    "\n",
    "### text = nfx.remove_userhandles(text):\n",
    "- This line uses the NeatText library to remove user handles (e.g., @username) from the text.\n",
    "\n",
    "### words = word_tokenize(text):\n",
    "- This line tokenizes the text into individual words using NLTK's word_tokenize function. Tokenization means breaking a text into smaller units, usually words or sentences.\n",
    "\n",
    "### lemmatized_words = [lemmatizer.lemmatize(word) for word in words]: \n",
    "- Here, each word in the tokenized text is lemmatized using the WordNetLemmatizer we created earlier. Lemmatization converts words to their base or root form, which helps in reducing variations of words.\n",
    "\n",
    "### cleaned_words = [word for word in lemmatized_words if len(word) > 2]:\n",
    "- This line creates a list of words after lemmatization, but it filters out words that are less than 3 characters long. This step removes short words that may not carry significant meaning.\n",
    "\n",
    "### return ' '.join(cleaned_words): \n",
    "- Finally, the function joins the cleaned words back into a single string with spaces between them and returns this processed text.\n",
    "\n",
    "### df['Clean_Text'] = df['Text'].apply(preprocess_text): \n",
    "- This line applies the preprocess_text function to each row of the 'Text' column in the DataFrame df and assigns the processed text to a new column named 'Clean_Text'.\n",
    "\n",
    "Overall, this code snippet demonstrates text preprocessing techniques such as removing user handles and short words, tokenization, and lemmatization using the NeatText library and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89bff967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31274d",
   "metadata": {},
   "source": [
    "The dir(nfx) command will display all the attributes and methods available in the neattext.functions module, which you imported as nfx. This can help you explore what functions are available for text preprocessing and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b8f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad6275",
   "metadata": {},
   "source": [
    "### df['Clean_Text']: \n",
    "- This part accesses the column named 'Clean_Text' in the DataFrame df. This column is likely to contain preprocessed text data.\n",
    "\n",
    "### df['Clean_Text'].apply(...): \n",
    "- Here, the apply method is used to apply a function to each element of the 'Clean_Text' column.\n",
    "\n",
    "### nfx.remove_stopwords:\n",
    "- This is a function from the NeatText library (nfx) called remove_stopwords. Stopwords are common words like 'the', 'is', 'and', etc., which are often removed during text preprocessing because they usually don't carry specific meaning.\n",
    "\n",
    "### df['Clean_Text'].apply(nfx.remove_stopwords):\n",
    "- By combining the above parts, this line applies the remove_stopwords function to each element (text) in the 'Clean_Text' column of the DataFrame, effectively removing stopwords from each piece of text.\n",
    "\n",
    "In summary, the code snippet removes stopwords from the 'Clean_Text' column in the DataFrame df, enhancing the quality of the text data for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32454d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional noise removal\n",
    "noise_removal_functions = [\n",
    "    'remove_accents',\n",
    "    'remove_bad_quotes',\n",
    "    'remove_emojis',\n",
    "    'remove_html_tags',\n",
    "    'remove_non_ascii',\n",
    "    'remove_puncts',\n",
    "    'remove_special_characters',\n",
    "    'remove_terms_in_bracket',\n",
    "    'remove_urls',\n",
    "]\n",
    "\n",
    "for func_name in noise_removal_functions:\n",
    "    df['Clean_Text'] = df['Clean_Text'].apply(getattr(nfx, func_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd286ae",
   "metadata": {},
   "source": [
    "### noise_removal_functions:\n",
    "- This is a list containing names of functions from the NeatText library (nfx) that are designed to remove specific types of noise or unwanted elements from text data. These functions include removing accents, bad quotes, emojis, HTML tags, non-ASCII characters, punctuation marks, special characters, terms in brackets, and URLs.\n",
    "\n",
    "### for func_name in noise_removal_functions:\n",
    "- This line starts a loop where each function name from the noise_removal_functions list is iterated over one by one.\n",
    "\n",
    "### getattr(nfx, func_name):\n",
    "- This is a Python built-in function that retrieves the function object by its name (func_name) from the NeatText library (nfx). It dynamically accesses the function based on the name provided.\n",
    "\n",
    "### df['Clean_Text'] = df['Clean_Text'].apply(getattr(nfx, func_name)):\n",
    "- In this line, the apply method is used to apply the function retrieved dynamically to each element (text) in the 'Clean_Text' column of the DataFrame df. This effectively applies multiple noise removal functions to the text data in a sequential manner, based on the functions listed in noise_removal_functions.\n",
    "\n",
    "In summary, the code snippet iterates through a list of noise removal functions and applies each function to the 'Clean_Text' column of the DataFrame df, allowing for comprehensive noise removal from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecdaf676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade list tommorow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>WAY HOMEGIRL BABY FUNERAL MAN HATE FUNERALS SH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye true hazel eyeand brilliant Regular featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh babe hugggzzz babe naamazed nga ako babe d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34787</th>\n",
       "      <td>surprise</td>\n",
       "      <td>@MichelGW have you gift! Hope you like it! It'...</td>\n",
       "      <td>gift Hope like hand wear ll warm Lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34788</th>\n",
       "      <td>joy</td>\n",
       "      <td>The world didnt give it to me..so the world MO...</td>\n",
       "      <td>world didnt world DEFINITELY cnt away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34789</th>\n",
       "      <td>anger</td>\n",
       "      <td>A man robbed me today .</td>\n",
       "      <td>man robbed today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34790</th>\n",
       "      <td>fear</td>\n",
       "      <td>Youu call it JEALOUSY, I call it of #Losing YO...</td>\n",
       "      <td>Youu JEALOUSY Losing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34791</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I think about you baby, and I dream about you ...</td>\n",
       "      <td>think baby dream time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34792 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                               Text  \\\n",
       "0       neutral                                             Why ?    \n",
       "1           joy    Sage Act upgrade on my to do list for tommorow.   \n",
       "2       sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3           joy   Such an eye ! The true hazel eye-and so brill...   \n",
       "4           joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "...         ...                                                ...   \n",
       "34787  surprise  @MichelGW have you gift! Hope you like it! It'...   \n",
       "34788       joy  The world didnt give it to me..so the world MO...   \n",
       "34789     anger                           A man robbed me today .    \n",
       "34790      fear  Youu call it JEALOUSY, I call it of #Losing YO...   \n",
       "34791   sadness  I think about you baby, and I dream about you ...   \n",
       "\n",
       "                                              Clean_Text  \n",
       "0                                                         \n",
       "1                         Sage Act upgrade list tommorow  \n",
       "2      WAY HOMEGIRL BABY FUNERAL MAN HATE FUNERALS SH...  \n",
       "3      eye true hazel eyeand brilliant Regular featur...  \n",
       "4      ugh babe hugggzzz babe naamazed nga ako babe d...  \n",
       "...                                                  ...  \n",
       "34787               gift Hope like hand wear ll warm Lol  \n",
       "34788              world didnt world DEFINITELY cnt away  \n",
       "34789                                   man robbed today  \n",
       "34790                              Youu JEALOUSY Losing   \n",
       "34791                              think baby dream time  \n",
       "\n",
       "[34792 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61757d",
   "metadata": {},
   "source": [
    "## Splitting data into input variables and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b12855",
   "metadata": {},
   "source": [
    "x: Features are the attributes and variables extracted from the dataset. These extracted features are used as inputs to the model during training.\n",
    "\n",
    "y: Labels are the output or the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2568bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Clean_Text']\n",
    "y = df['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153c41a",
   "metadata": {},
   "source": [
    "We need to split our dataset into a train set and test set. The model will learn from the train set. We will use the test set to evaluate the model performance and measure the modelâ€™s knowledge capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0781e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7b6e",
   "metadata": {},
   "source": [
    "### from sklearn.model_selection import train_test_split: \n",
    "- This line imports the train_test_split function from the sklearn.model_selection module. This function is commonly used to split data into training and testing sets for machine learning tasks.\n",
    "\n",
    "### x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42): \n",
    "- This line actually performs the data split:\n",
    "\n",
    "- -  x and y are your input features and target labels, respectively.\n",
    "test_size=0.3 indicates that 30% of the data will be used for testing, and the remaining 70% will be used for training.\n",
    "random_state=42 sets the random seed for reproducibility. This ensures that each time you run the split, you get the same random data points in the training and testing sets, which is useful for consistent results during development and testing.\n",
    "After executing this code, you'll have four sets of data:\n",
    "\n",
    "- x_train: The input features for training your model.\n",
    "- x_test: The input features for testing your model.\n",
    "- y_train: The corresponding target labels for training.\n",
    "- y_test: The corresponding target labels for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94068470",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab70dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d3bb4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy: 0.62\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.55      0.58      1283\n",
      "     disgust       0.57      0.16      0.25       292\n",
      "        fear       0.75      0.65      0.70      1645\n",
      "         joy       0.62      0.76      0.69      3311\n",
      "     neutral       0.57      0.71      0.63       675\n",
      "     sadness       0.59      0.57      0.58      2015\n",
      "       shame       0.83      0.81      0.82        36\n",
      "    surprise       0.56      0.43      0.48      1181\n",
      "\n",
      "    accuracy                           0.62     10438\n",
      "   macro avg       0.64      0.58      0.59     10438\n",
      "weighted avg       0.62      0.62      0.62     10438\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 709    7   61  230   81  155    1   39]\n",
      " [  38   47   26   87    8   58    1   27]\n",
      " [  84    3 1074  231   43  139    0   71]\n",
      " [ 114    5  122 2520  123  250    1  176]\n",
      " [  15    0   10  106  482   54    0    8]\n",
      " [ 130   12   94  478   68 1148    2   83]\n",
      " [   0    0    1    6    0    0   29    0]\n",
      " [  59    8   45  387   44  132    1  505]]\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with CountVectorizer and Logistic Regression\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_lr.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = pipe_lr.score(x_test, y_test)\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = pipe_lr.predict(x_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdaec86",
   "metadata": {},
   "source": [
    "###### Create the Pipeline:\n",
    "- A pipeline named pipe_lr is created using Pipeline from scikit-learn.\n",
    "###### The pipeline consists of two steps:\n",
    "- cv: CountVectorizer, which converts text data into a matrix of token counts.\n",
    "- lr: LogisticRegression, a classification algorithm used for binary and multi-class classification tasks.\n",
    "###### Fit the Pipeline on Training Data:\n",
    "- It trains both the CountVectorizer and Logistic Regression model together.\n",
    "###### Evaluate the Pipeline on Test Data:\n",
    "- The pipeline's accuracy is evaluated on the test data (x_test and y_test) using the score method, which calculates the accuracy of the model predictions.The accuracy value is then printed.\n",
    "###### Make Predictions:\n",
    "- This line makes predictions on the test data using the fitted pipeline.\n",
    "###### Print Classification Report:\n",
    "- The classification report provides a summary of various metrics such as precision, recall, F1-score, and support for each class in the classification task.\n",
    "\n",
    "It helps evaluate the model's performance in detail for each class.\n",
    "\n",
    "###### Print Confusion Matrix:\n",
    "- The confusion matrix is a table that describes the performance of a classification model. It shows the true positive, false positive, true negative, and false negative values, helping to understand how well the model is predicting each class.\n",
    "\n",
    "Overall, this code is used to train a text classification model using CountVectorizer to convert text data into numerical features and Logistic Regression as the classifier. The model is then evaluated using accuracy, classification report, and confusion matrix to assess its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b6b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy: 0.62\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.53      0.56      1283\n",
      "     disgust       0.62      0.11      0.19       292\n",
      "        fear       0.75      0.68      0.71      1645\n",
      "         joy       0.60      0.77      0.68      3311\n",
      "     neutral       0.65      0.73      0.69       675\n",
      "     sadness       0.59      0.55      0.57      2015\n",
      "       shame       0.78      0.58      0.67        36\n",
      "    surprise       0.59      0.41      0.48      1181\n",
      "\n",
      "    accuracy                           0.62     10438\n",
      "   macro avg       0.65      0.55      0.57     10438\n",
      "weighted avg       0.62      0.62      0.61     10438\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 684    4   63  270   56  167    1   38]\n",
      " [  45   33   19  115    5   52    1   22]\n",
      " [  80    1 1113  260   28  119    0   44]\n",
      " [ 102    5  122 2560  113  266    1  142]\n",
      " [  34    0    9   86  495   45    0    6]\n",
      " [ 155    6  102  525   40 1108    2   77]\n",
      " [   2    0    1    9    0    2   21    1]\n",
      " [  67    4   52  421   30  127    1  479]]\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with CountVectorizer and SVM\n",
    "pipe_svm = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('svc', SVC(kernel='rbf', C=10))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_svm.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = pipe_svm.score(x_test, y_test)\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = pipe_svm.predict(x_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a76eb",
   "metadata": {},
   "source": [
    "This code segment creates a machine learning pipeline using CountVectorizer for text vectorization and Support Vector Machine (SVM) with a radial basis function (RBF) kernel and C=10 for classification. Here's a breakdown of each step:\n",
    "### Create Pipeline:\n",
    "###### pipe_svm = Pipeline(steps=[('cv', CountVectorizer()), ('svc', SVC(kernel='rbf', C=10))]): \n",
    "- This line creates a pipeline with two steps. The first step is CountVectorizer, which converts text data into a numerical format suitable for machine learning. The second step is SVC (Support Vector Classifier) with an RBF (radial basis function) kernel and C=10, which is a hyperparameter controlling the regularization strength.\n",
    "### Fit Pipeline:\n",
    "###### pipe_svm.fit(x_train, y_train): \n",
    "- This line fits the pipeline on the training data (x_train) with corresponding target labels (y_train). This process trains the CountVectorizer on the text data and trains the SVM classifier on the transformed numerical features.\n",
    "### Evaluate Pipeline:\n",
    "###### accuracy = pipe_svm.score(x_test, y_test):\n",
    "- This line evaluates the accuracy of the pipeline on the test data (x_test) and corresponding target labels (y_test). The accuracy score represents the proportion of correctly predicted instances.\n",
    "### Make Predictions:\n",
    "###### svm_predictions = pipe_svm.predict(x_test): \n",
    "- After fitting the pipeline, this line makes predictions on the test data using the trained SVM model.\n",
    "### Print Results:\n",
    "###### print(f\"Pipeline Accuracy: {accuracy:.2f}\"):\n",
    "- Prints the accuracy of the pipeline on the test data.\n",
    "###### print(\"\\nClassification Report:\"): \n",
    "- Prints the classification report, which includes metrics such as precision, recall, F1-score, and support for each class.\n",
    "###### print(classification_report(y_test, svm_predictions)): \n",
    "- Prints the classification report based on the actual labels (y_test) and predicted labels (svm_predictions).\n",
    "###### print(\"\\nConfusion Matrix:\"): \n",
    "- Prints the confusion matrix, which shows the count of true positives, true negatives, false positives, and false negatives.\n",
    "###### print(confusion_matrix(y_test, svm_predictions)): \n",
    "- Prints the confusion matrix based on the actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46e9ce98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy: 0.56\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.48      0.52      1283\n",
      "     disgust       0.52      0.12      0.19       292\n",
      "        fear       0.78      0.61      0.69      1645\n",
      "         joy       0.60      0.69      0.64      3311\n",
      "     neutral       0.29      0.78      0.42       675\n",
      "     sadness       0.53      0.49      0.51      2015\n",
      "       shame       0.83      0.69      0.76        36\n",
      "    surprise       0.68      0.30      0.41      1181\n",
      "\n",
      "    accuracy                           0.56     10438\n",
      "   macro avg       0.60      0.52      0.52     10438\n",
      "weighted avg       0.60      0.56      0.56     10438\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 616    7   48  231  182  182    1   16]\n",
      " [  31   35   14   91   51   59    1   10]\n",
      " [  77    7 1006  217  157  148    0   33]\n",
      " [ 116    6   87 2273  458  298    1   72]\n",
      " [  21    1   13   62  525   51    0    2]\n",
      " [ 145    4   73  492  273  991    1   36]\n",
      " [   3    0    0    1    1    6   25    0]\n",
      " [  71    7   46  407  175  123    1  351]]\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with CountVectorizer and Random Forest Classifier\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=10))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_rf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = pipe_rf.score(x_test, y_test)\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = pipe_rf.predict(x_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b3ab8",
   "metadata": {},
   "source": [
    "This code creates a machine learning pipeline using CountVectorizer for text vectorization and a Random Forest Classifier for classification. Here's a breakdown of each step and what it does:\n",
    "\n",
    "### Create Pipeline:\n",
    "###### pipe_rf = Pipeline(steps=[('cv', CountVectorizer()), ('rf', RandomForestClassifier(n_estimators=10))]):\n",
    "- This line creates a pipeline with two steps. The first step is CountVectorizer, which converts text data into a numerical format suitable for machine learning. The second step is RandomForestClassifier with 10 estimators, which is the number of decision trees in the random forest.\n",
    "### Fit Pipeline:\n",
    "###### pipe_rf.fit(x_train, y_train):\n",
    "- This line fits the pipeline on the training data (x_train) with corresponding target labels (y_train). This process trains the CountVectorizer on the text data and trains the Random Forest Classifier on the transformed numerical features.\n",
    "### Evaluate Pipeline:\n",
    "###### accuracy = pipe_rf.score(x_test, y_test):\n",
    "- This line evaluates the accuracy of the pipeline on the test data (x_test) and corresponding target labels (y_test). The accuracy score represents the proportion of correctly predicted instances.\n",
    "### Make Predictions:\n",
    "###### rf_predictions = pipe_rf.predict(x_test):\n",
    "- After fitting the pipeline, this line makes predictions on the test data using the trained Random Forest Classifier.\n",
    "### Print Results:\n",
    "###### print(f\"Pipeline Accuracy: {accuracy:.2f}\"): \n",
    "- Prints the accuracy of the pipeline on the test data.\n",
    "###### print(\"\\nClassification Report:\"):\n",
    "- Prints the classification report, which includes metrics such as precision, recall, F1-score, and support for each class.\n",
    "###### print(classification_report(y_test, rf_predictions)): \n",
    "- Prints the classification report based on the actual labels (y_test) and predicted labels (rf_predictions).\n",
    "###### print(\"\\nConfusion Matrix:\"): \n",
    "- Prints the confusion matrix, which shows the count of true positives, true negatives, false positives, and false negatives.\n",
    "###### print(confusion_matrix(y_test, rf_predictions)): \n",
    "- Prints the confusion matrix based on the actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40a3298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      0.49      0.57      1283\n",
      "     disgust       0.84      0.13      0.22       292\n",
      "        fear       0.83      0.61      0.70      1645\n",
      "         joy       0.60      0.72      0.66      3311\n",
      "     neutral       0.30      0.79      0.43       675\n",
      "     sadness       0.56      0.53      0.55      2015\n",
      "       shame       0.86      0.86      0.86        36\n",
      "    surprise       0.71      0.33      0.45      1181\n",
      "\n",
      "    accuracy                           0.58     10438\n",
      "   macro avg       0.67      0.56      0.55     10438\n",
      "weighted avg       0.64      0.58      0.58     10438\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 634    0   35  235  183  183    1   12]\n",
      " [  27   37    6   99   55   56    1   11]\n",
      " [  58    2 1000  250  165  138    0   32]\n",
      " [  81    1   59 2395  439  268    1   67]\n",
      " [  17    0    9   71  535   42    0    1]\n",
      " [  94    1   55  504  260 1065    1   35]\n",
      " [   1    0    0    2    0    2   31    0]\n",
      " [  38    3   42  416  156  134    1  391]]\n",
      "\n",
      "Extra Trees Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.64      0.51      0.57      1283\n",
      "     disgust       0.78      0.17      0.28       292\n",
      "        fear       0.83      0.61      0.70      1645\n",
      "         joy       0.61      0.73      0.66      3311\n",
      "     neutral       0.33      0.78      0.47       675\n",
      "     sadness       0.55      0.53      0.54      2015\n",
      "       shame       0.88      0.81      0.84        36\n",
      "    surprise       0.69      0.36      0.47      1181\n",
      "\n",
      "    accuracy                           0.59     10438\n",
      "   macro avg       0.66      0.56      0.57     10438\n",
      "weighted avg       0.63      0.59      0.59     10438\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 648    2   38  234  143  196    1   21]\n",
      " [  23   50    8   94   47   57    1   12]\n",
      " [  67    3 1006  244  131  144    0   50]\n",
      " [  90    3   64 2423  366  295    0   70]\n",
      " [  20    0    8   77  527   42    0    1]\n",
      " [ 112    3   55  505  237 1063    1   39]\n",
      " [   6    0    0    0    0    1   29    0]\n",
      " [  43    3   36  406  130  136    1  426]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(x_train_vectorized, y_train)\n",
    "rf_predictions = rf_classifier.predict(x_test_vectorized)\n",
    "\n",
    "# Extra Trees Classifier\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_classifier.fit(x_train_vectorized, y_train)\n",
    "et_predictions = et_classifier.predict(x_test_vectorized)\n",
    "\n",
    "# Evaluate Random Forest Classifier\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "# Evaluate Extra Trees Classifier\n",
    "print(\"\\nExtra Trees Classifier:\")\n",
    "print(classification_report(y_test, et_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, et_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce75ca9",
   "metadata": {},
   "source": [
    "This code performs text classification using Random Forest Classifier and Extra Trees Classifier after vectorizing the text data with CountVectorizer. Here's a step-by-step explanation of the code:\n",
    "\n",
    "### Import Libraries:\n",
    "###### from sklearn.feature_extraction.text import CountVectorizer:\n",
    "- Imports CountVectorizer from scikit-learn, which is used for converting text data into numerical features.\n",
    "###### from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier:\n",
    "- Imports RandomForestClassifier and ExtraTreesClassifier, which are ensemble tree-based classification algorithms.\n",
    "###### from sklearn.metrics import classification_report, confusion_matrix:\n",
    "- Imports functions for generating classification reports and confusion matrices.\n",
    "### Vectorize Text Data: \n",
    "###### vectorizer = CountVectorizer(): \n",
    "- Initializes a CountVectorizer object.\n",
    "###### x_train_vectorized = vectorizer.fit_transform(x_train):\n",
    "- Fits the vectorizer on the training text data (x_train) and transforms it into a numerical format.\n",
    "###### x_test_vectorized = vectorizer.transform(x_test): \n",
    "- Transforms the test text data (x_test) using the fitted vectorizer from the training data.\n",
    "### Random Forest Classifier:\n",
    "###### rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42): \n",
    "- Initializes a Random Forest Classifier with 100 estimators (trees) and sets a random state for reproducibility.\n",
    "###### rf_classifier.fit(x_train_vectorized, y_train): \n",
    "- Fits the Random Forest Classifier on the vectorized training data and corresponding labels (y_train).\n",
    "###### rf_predictions = rf_classifier.predict(x_test_vectorized):\n",
    "- Makes predictions on the vectorized test data using the trained Random Forest Classifier.\n",
    "### Extra Trees Classifier:\n",
    "###### et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42): \n",
    "- Initializes an Extra Trees Classifier with 100 estimators and a random state.\n",
    "###### et_classifier.fit(x_train_vectorized, y_train): \n",
    "- Fits the Extra Trees Classifier on the vectorized training data and labels.\n",
    "###### et_predictions = et_classifier.predict(x_test_vectorized): \n",
    "- Makes predictions on the vectorized test data using the trained Extra Trees Classifier.\n",
    "### Evaluation:\n",
    "- Prints the classification report and confusion matrix for both the Random Forest Classifier and Extra Trees Classifier using the test data.\n",
    "- The classification report includes metrics such as precision, recall, F1-score, and support for each class.\n",
    "- The confusion matrix shows the counts of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92183b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy: 0.57\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.66      0.50      0.57      1283\n",
      "     disgust       0.62      0.02      0.03       292\n",
      "        fear       0.74      0.62      0.67      1645\n",
      "         joy       0.51      0.86      0.64      3311\n",
      "     neutral       0.43      0.02      0.03       675\n",
      "     sadness       0.56      0.55      0.55      2015\n",
      "       shame       0.00      0.00      0.00        36\n",
      "    surprise       0.67      0.27      0.38      1181\n",
      "\n",
      "    accuracy                           0.57     10438\n",
      "   macro avg       0.52      0.35      0.36     10438\n",
      "weighted avg       0.59      0.57      0.53     10438\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 645    0   53  370    4  202    0    9]\n",
      " [  29    5   31  130    0   86    0   11]\n",
      " [  67    2 1019  346    1  163    0   47]\n",
      " [  64    0  111 2859    7  217    0   53]\n",
      " [  22    0   22  545   12   70    0    4]\n",
      " [  98    0   85  695    2 1105    0   30]\n",
      " [  11    0    6   14    0    5    0    0]\n",
      " [  44    1   58  623    2  140    0  313]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with CountVectorizer and Naive Bayes Classifier\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_nb.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = pipe_nb.score(x_test, y_test)\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "nb_predictions = pipe_nb.predict(x_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61222c9",
   "metadata": {},
   "source": [
    "This code snippet creates a pipeline using CountVectorizer and the Multinomial Naive Bayes Classifier for text classification tasks. Here's a breakdown of what each part does:\n",
    "\n",
    "### Create Pipeline:\n",
    "###### pipe_nb = Pipeline(steps=[('cv', CountVectorizer()), ('nb', MultinomialNB())]):\n",
    "- Defines a pipeline with two steps:\n",
    "- - 'cv': CountVectorizer step for text vectorization.\n",
    "- - 'nb': Multinomial Naive Bayes Classifier step for classification.\n",
    "###Fit and Evaluate:\n",
    "###### pipe_nb.fit(x_train, y_train):\n",
    "- Fits the pipeline on the training data (x_train) and corresponding labels (y_train).\n",
    "###### accuracy = pipe_nb.score(x_test, y_test):\n",
    "- Evaluates the pipeline's accuracy on the test data (x_test) and labels (y_test).\n",
    "###### print(f\"Pipeline Accuracy: {accuracy:.2f}\"): \n",
    "- Prints the pipeline's accuracy.\n",
    "### Make Predictions:\n",
    "###### nb_predictions = pipe_nb.predict(x_test):\n",
    "- Makes predictions using the fitted pipeline on the test data.\n",
    "### Print Evaluation Metrics:\n",
    "###### print(\"\\nClassification Report:\"):\n",
    "- Prints the classification report, which includes precision, recall, F1-score, and support for each class.\n",
    "###### print(classification_report(y_test, nb_predictions))\n",
    "###### print(\"\\nConfusion Matrix:\"): \n",
    "- Prints the confusion matrix, showing true positives, true negatives, false positives, and false negatives.\n",
    "###### print(confusion_matrix(y_test, nb_predictions)):\n",
    "- The pipeline simplifies the workflow by combining text vectorization and classification into a single object, making it easier to train, evaluate, and use the model for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac497a",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed05d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "pipeline_file = open(\"text_emotion.pkl\",\"wb\")\n",
    "joblib.dump(pipe_lr,pipeline_file)\n",
    "pipeline_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5961716",
   "metadata": {},
   "source": [
    "This code snippet involves using the joblib library to save a machine learning pipeline to a file. Here's a detailed explanation:\n",
    "\n",
    "### Import Libraries:\n",
    "###### import joblib: \n",
    "- Imports the joblib library, which provides utilities for saving and loading Python objects, especially machine learning models.\n",
    "### Define Pipeline:\n",
    "###### pipeline_file = open(\"text_emotion.pkl\",\"wb\"): \n",
    "- Opens a file named \"text_emotion.pkl\" in binary write mode (\"wb\").\n",
    "###### joblib.dump(pipe_lr,pipeline_file): \n",
    "- Uses the joblib's dump function to save the pipeline (pipe_lr) to the opened file (pipeline_file).\n",
    "###### pipe_lr:\n",
    "- This is a pre-trained pipeline object containing text preprocessing and a machine learning model.\n",
    "###### pipeline_file: \n",
    "- This is the file object where the pipeline will be saved.\n",
    "### Close File:\n",
    "###### pipeline_file.close(): \n",
    "- Closes the file after saving the pipeline.\n",
    "### Explanation in Steps:\n",
    "#### Serialization:\n",
    "- The purpose of this code is to serialize (or save) a trained machine learning pipeline. Serialization refers to converting the object (pipeline in this case) into a format that can be stored in a file or transferred over a network.\n",
    "\n",
    "#### Joblib Library: \n",
    "- Joblib is used here because it's well-suited for serializing large NumPy arrays efficiently. It's commonly used in the Python machine learning ecosystem for saving and loading models.\n",
    "\n",
    "#### File Opening and Writing:\n",
    "###### open(\"text_emotion.pkl\",\"wb\"): \n",
    "- Opens a file named \"text_emotion.pkl\" in binary write mode. The \"wb\" mode ensures that the file is opened for writing in binary mode, which is necessary for non-text data like machine learning models.\n",
    "###### joblib.dump(pipe_lr, pipeline_file): \n",
    "- The dump function from joblib writes the contents of pipe_lr (the trained pipeline) to the file pipeline_file.\n",
    "#### Closing the File:\n",
    "###### pipeline_file.close():\n",
    "- Closes the file after writing the pipeline. It's important to close files after writing to ensure that all data is properly flushed and the file resources are released.\n",
    "\n",
    "#### After running this code, you'll have a file named \"text_emotion.pkl\" containing the serialized version of your machine learning pipeline (pipe_lr). This file can be later loaded using joblib's load function to restore the pipeline and make predictions without retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958d4f5",
   "metadata": {},
   "source": [
    "#### I used the logistic regression model (pipe_lr) because it achieved the highest accuracy among all the models you tested. In the classification report and confusion matrix I provided, the logistic regression model achieved an accuracy of 0.62, which is the highest among the models you mentioned. The precision, recall, and F1-score metrics for each emotion category also seem balanced and acceptable, considering the complexity of the emotion classification task.\n",
    "\n",
    "#### Here are some key points supporting why you used the logistic regression model:\n",
    "###### High Accuracy: \n",
    "- The accuracy of 0.62 indicates that the logistic regression model performed well in predicting the emotions in your dataset.\n",
    "\n",
    "###### Balanced Metrics: \n",
    "- The precision, recall, and F1-score metrics for each emotion category are reasonably balanced, showing that the model's predictions are consistent across different classes.\n",
    "\n",
    "###### Interpretability: \n",
    "- Logistic regression models are relatively easy to interpret compared to more complex models like SVMs or neural networks. This can be beneficial for understanding which features contribute most to the predictions.\n",
    "\n",
    "###### Efficiency: \n",
    "- Logistic regression models are computationally efficient and can handle large datasets relatively well, making them suitable for practical applications.\n",
    "\n",
    "#### Overall, based on the high accuracy and balanced metrics achieved by the logistic regression model, it seems like a reasonable choice for emotion classification in your scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
